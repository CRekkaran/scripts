Trying to get a handle on how to best train affinity.

Variables under consideration:
gapped loss: 0, 1, 2
pose type (input data):
    A. crystal structs "crystal"  nobalance
    B. best rmsd docked "bestonly" nobalance
    C. best rmsd w/negated all other  (2X multiple) "besty" - needs reduced
    D. <2 rmsd  (nX multiple)  "posonly" nobalance - doesn't hurt to have reduced, n~4
    E. <2 rmsd w/negated highrmsd (nX multiple)  "all" - needs reduced, n~4
pose sampling (always balance):
    balanced if there are negative examples (but otherwise uniform)
    receptor stratified (not applicable to A or B)
    affinity stratified
    receptor and affinity stratified
pose prediction
    none (affonly)
    included (not applicable to A, B, or D)


Should also re-evaluate traditional hyper-parameters.
Consider slightly deeper/wider models.

Evaluation:
kind: RMSE, Spearman, and Pearson
test set: crystal only, best rmsd only, all poses (highest prediction)

Run predict using best model and best model after normalized # of iterations


#include minified poses
cat pdbbind.train cnnmin.types cnnmin2.types > affinity_search/all.types

../addaffinities.py all.types ../refinedSet.txt 

#Create new train/test sets:

~/git/gninascripts/compute_seqs.py --pdbfiles pdbinfo --out pdb.seqs
# distribute the row computation
~/git/gninascripts/compute_row.py --pdbseqs pdb.seqs -r 0 --out seqdist/row.0
~/git/gninascripts/combine_rows.py seqrow.*
 

~/git/gninascripts/clustering.py -i crystal.types --cpickle matrix.pickle -s 0.5 -v -d .. -o crystal_0.5_0_
for i in {1..4} 
do 
~/git/gninascripts/clustering.py -i crystal.types --cpickle matrix.pickle -s 0.5 -v -d .. -o crystal_0.5_${i}_ --randomize $i
done

~/git/gninascripts/clustering.py -i all.types --cpickle matrix.pickle -s 0.5 -v -d .. -o all_0.5_0_
for i in {1..4} 
do 
~/git/gninascripts/clustering.py -i all.types --cpickle matrix.pickle -s 0.5 -v -d .. -o all_0.5_${i}_ --randomize $i
done

#posonly  training set
for i in all_*.types; do  awk '$1 > 0 {print $0}' $i > posonly${i#all}; done

#bestonly
for i in posonly_0.5_*; do sort -n -k6,6 $i | sort -s -k3,3 -u > bestonly${i#posonly}; done

#besty
for i in all_*; do ./makebesty.py $i > besty${i#all}; done

for i in *_test*.types
do
 n=`echo $i | sed 's/_test/__reducedtest/'`
 ./makereduced.py $i > $n
done

for i in *_train*.types
do
 n=`echo $i | sed 's/_train/__reducedtrain/'`
 ./makereduced.py $i > $n
done

# use the default (0) split for hyper parameter optimization

# should eventually measure variance of different random seed vs. different training set

# plan is to evaluate all configurations on the pareto optimal curve with
# multiple train splits to determine statistical significance of different accuracies

./makemodels.py > cmds

#evaluate on crystal, bestonly, and all

for i in `awk '{print $NF}' cmds`; do echo ./evaluate.py $i ${i}.summary; done > evals

#analysis of summary files is in summaries.ipynb
#Some observations:
#We may beoverfitting at 100k iterations (best25 typically does best).  This is significant for the smaller training sets.  However, for all/all the best results are often at 100k
#stratification hurts RMSE, but not correlation (maybe very slightly better correlation)
#receptor strat seems to be very slighly better
#training on all does best; bestonly/crystal do poorly when tested on all
#gap generally improves rmse, but not for the best rmse; doesn't help correlation
#pose scoring doesn't seem to have a strong effect one way or another
#best results are achieved testing crystal structures
#affinity selection seems to outperform pose selection

# best all/all rmse:
# all	g1	p1	rec1	astrat0	b1	all_affinity	best100	1.664972	0.565712	0.565194
# best all/all R:
# all	g1	p1	rec0	astrat0	b1	all_affinity	100k	1.688566	0.568719	0.570501
# all	g2	p1	rec1	astrat1	b1	all_affinity	100k	1.693725	0.568635	0.572275

#investigate pseudo huber loss
for i in *h?.model; do echo train.py -m $i  -p all_0.5_0_ --keep_best -t 1000 -i 100000 --reduced -o all_${i%.model}; done

train.py -m affinity_g0_p0_rec1_astrat0_b1_h1.model -p all_0.5_0_ --keep_best -t 1000 -i 100000 --reduced -o all_affinity_g0_p0_rec1_astrat0_b1_h1
train.py -m affinity_g0_p0_rec1_astrat0_b1_h2.model -p all_0.5_0_ --keep_best -t 1000 -i 100000 --reduced -o all_affinity_g0_p0_rec1_astrat0_b1_h2
train.py -m affinity_g0_p0_rec1_astrat0_b1_h4.model -p all_0.5_0_ --keep_best -t 1000 -i 100000 --reduced -o all_affinity_g0_p0_rec1_astrat0_b1_h4
train.py -m affinity_g0_p1_rec1_astrat0_b1_h1.model -p all_0.5_0_ --keep_best -t 1000 -i 100000 --reduced -o all_affinity_g0_p1_rec1_astrat0_b1_h1
train.py -m affinity_g0_p1_rec1_astrat0_b1_h2.model -p all_0.5_0_ --keep_best -t 1000 -i 100000 --reduced -o all_affinity_g0_p1_rec1_astrat0_b1_h2
train.py -m affinity_g0_p1_rec1_astrat0_b1_h4.model -p all_0.5_0_ --keep_best -t 1000 -i 100000 --reduced -o all_affinity_g0_p1_rec1_astrat0_b1_h4

#h4 is best performing so far, but h6 and h8 fail to converge consistently

train.py -m affinity_g0_p1_rec1_astrat0_b1_h6.model -p all_0.5_0_ --keep_best -t 1000 -i 100000 --reduced -o all_affinity_g0_p1_rec1_astrat0_b1_h6
train.py -m affinity_g0_p1_rec1_astrat0_b1_h8.model -p all_0.5_0_ --keep_best -t 1000 -i 100000 --reduced -o all_affinity_g0_p1_rec1_astrat0_b1_h8
train.py -m affinity_g0_p1_rec1_astrat1_b1_h4.model -p all_0.5_0_ --keep_best -t 1000 -i 100000 --reduced -o all_affinity_g0_p1_rec1_astrat1_b1_h4
train.py -m affinity_g2_p1_rec1_astrat0_b1_h4.model -p all_0.5_0_ --keep_best -t 1000 -i 100000 --reduced -o all_affinity_g2_p1_rec1_astrat0_b1_h4
train.py -m affinity_g0_p1_rec1_astrat0_b1_h4_y1.model -p all_0.5_0_ --keep_best -t 1000 -i 100000 --reduced -o all_affinity_g0_p1_rec1_astrat0_b1_h4_y1
train.py -m affinity_g0_p1_rec1_astrat0_b1_h4_y2.model -p all_0.5_0_ --keep_best -t 1000 -i 100000 --reduced -o all_affinity_g0_p1_rec1_astrat0_b1_h4_y2

#Note: at this point I realized that  2cer and 2gyi were both incorrectly being included (0 affinity) and removed them

#investigating effect of only training on extrema - can we get a better slope on the regression line?
cat all.types | awk '{print sqrt($2*$2),$0}' | sort -n | tail -24999 | head
cat all.types | awk '{print sqrt($2*$2),$0}' | sort -n | tail -24999 >> extrema.types 
awk '{print $2,$3,$4,$5,$6,$7,$8}' extrema.types > e
mv e extrema.types
~/git/gninascripts/clustering.py -i extrema.types --cpickle matrix.pickle  -s 0.5 -v -d .. -o extrema_0.5_0_

train.py -m affinity_g0_p1_rec1_astrat0_b1_h4.model -p extrema_0.5_0_ --keep_best -t 1000 -i 100000 -o extrema_affinity_g0_p1_rec1_astrat0_b1_h4

# more analysis in summaries.ipynb
# basically, training set is fitting just fine :-(

### BREAK: realized model didn't include Boron (instead had Bromine twice)
# archive everything to firstattempt

#Plan: 
# iteration1: vary training approach (training set, poses, rec strat, aff strat)
# iteration2: build on that (presumably all,p1,rec1,strat0) with loss modifications: h[1,2,4,6], penalty[0,1,2], gap[0,1,2], rankloss??
# iteration3: activation functions and normalizations

cd iteration1
./makemodels1.py > cmds1

for i in `awk '{print $NF}' cmds1`; do echo ../evaluate.py $i ${i}.summary; done > evals1

# all,poses, rec1, strat2

cd iteration2
./makemodels2.py  > cmds2
for i in `awk '{print $NF}' cmds2`; do echo ../evaluate.py $i ${i}.summary; done > evals2

# no penalty, gap=1, huber=0 (surprise!)


#normalization, activation function, and learning rate
# NOTE: switched to fixed sized conv layers here (accidentally actually) and got a little worse
cd iteration3
./makemodels3.py  > cmds3
for i in `awk '{print $NF}' cmds3`; do echo ../evaluate.py $i ${i}.summary; done > evals

# .1 lr is horrible, 0.001 too slow
# sigmoid is bad, others are about the same, tanh slightly worse, elu particularly good on crystals(?)
# batch generally worse, although sometimes better, lrn slightly better, but particularly good on crystals
# switch to elu and lrn, at least until we consider speed penalty


cd iteration4
# variables: 
# kernel size: 3, 5, 7
# depth: 2, 3, 4
# width: 16, 32, 64, 128
# doubling of width: true/false (e.g, 128->256->512)

./makemodels4.py  > cmds
for i in `awk '{print $NF}' cmds`; do echo ../evaluate.py $i ${i}.summary; done > evals

# depth 3 with width 32 is best (16 might be okay), but doubling doesn't seem to be such a good idea (consider narrowing??)
# ksize of 7 does best :-(


# while waiting for these larger models to train... implemented radial weight filling
# evaluate different weight initialization (using depth 3, width 32/64/128)
# initialization: gaussian, positive_unitball, uniform, xavier, msra, radial
# since the symmetry gets blown away in the first step, my guess is radial won't matter, so not bothering with fraction
# kernel size: 3, 5
cd iteration4.5
./makemodels4.5.py > cmds
for i in `awk '{print $NF}' cmds`; do echo ../evaluate.py $i ${i}.summary; done > evals

#xavier generally does best, positive_unitball can work too
#ksize=5 is no good, but didn't run on non-radial

#iteration 4 is still running..
#interested in looking at the solver, rankloss, and baselr (give up on radial)
#(adam seems to be a bit more robust)
# [SGD|Adam] * [ranklosswneg 0,1] [ranklossm - 0, 0.01,0.1,1] [0.01|0.001]
cd iteration4.6
./makemodels4.6.py > cmds
for i in `awk '{print $NF}' cmds`; do echo ../evaluate.py $i ${i}.summary; done > evals

#rankloss didn't help - large values could destabilize training, smaller values were the same or worse; larger values did push slope upwards at the expense of rmse
#rankneg either didn't have a big difference or made things definitively worse
#solver could go either way - sometimes meaningfully better, sometimes worse; adam might be a bit more robust
#similar ambivalence for lr 
#conclusion - don't change training regime
#alt conclusion, on further analysis, maybe switch to adam solver - train performance is improved and it appears to be less sensitive to hyperparameter values
#however, SGD has better test at 100k - perhaps adam is learning faster and overfitting is starting - adam's best values are at much earlier iterations than sgd
#so we could probably switch to Adam and 50k iterations (but didn't do this for iteration 5)

# iteration 5 - need to further investigate 3 vs 7 kernel sizes, can we have a stride? stride vs pool? narrowing of features
# kernel size: 3 or 7
# width: [32,32,32] [64,32,32] [64,32,16] [32,16,16]
# pool or not
# stride 1,2,3 (>1 if no pool)

./makemodels5.py > cmds
for i in `awk '{print $NF}' cmds`; do echo ../evaluate.py $i ${i}.summary; done > evals

#with ksize=3, 32-32-32 does well, 32-16-16 is slightly better at 100k (less overfitting)
#a stride of 2 (no pool) can do okay, but pooling is generally better
#default: affinity_32-32-32_3_1_1.model


# iteration 6 - evaluate alternative initial layers, all of which generate the same dimension as 2x2 pooling on a .5A grid
# resolutions: 0.125, 0.25, 0.5, 1.0
# reductions: pooling, strided convolution
# grid resolution:  0.25, 0.5, 1.0  # skipping 0.125 since the input size is just too large
# convolution: none, 2x2 (stride 2), 4x4 (stride 4), 8x8 (stride 8)
#  convolution can have activation function after it or not, be grouped or not
# max pooling: none, 2x2, 4x4, 8x8
# above are combined to generate 1A grid for next layer
# NOTE: have to use a reduced batch size (20)
# Double NOTE: initially had a bug where func1 wasn't doing anything and had to rerun

./makemodels6.py > cmds
for i in `awk '{print $NF}' cmds`; do echo ../evaluate.py $i ${i}.summary; done > evals

# 0.25 resolution -> conv2 -> pool2 (grouped) does best
# clear signal that 0.25 is better for pose prediction
# initial convolution better for pose prediction
# swapping genearlly a little worse, but can go either way; worse for pose prediction
# grouped is less clear, probably a little better, but can be much worse
# similar for func
# should evaluate cost of switching to grouped, no func conv instead of max pool
# our default is best for pose prediction at 0.5 (unsurprisingly?)
# but a 4x convolution is best for 0.25


# TAKING STOCK
# looking across all the iterations (summaries.ipynb) we haven't actually been
# pushing the pareto frontier forward since iteration 2/3  :-(
# perhaps elu/lrn was a poor choice?
# best pose prediction is clearly with 0.25, even with smaller batch size

#in any event, looking at the pareto optimal points (at 100k for all_affinity), evaluate the following
#for statistical difference:

# iteration2-g1_p0_h0
# iteration3-batch_relu_lr0.010
# iteration6-0.250_conv4_pool1_grouped0_func1_swap0   - needs to be trained 250k

cd check1
cp ../iteration2/affinity_g1_p0_h0.model .
cp ../iteration3/affinity_batch_relu.model .
cp ../iteration6/affinity_0.250_conv4_pool1_grouped0_func1_swap0.model .

# Basically - it's all in the noise.  Pose prediciton (top) improvement is statistically significatn for 0.250
# Affinity predicition (correlation) _might_ be _barely_ significant for initial model

# evaluate variation due to random seed vs testset
cd check2
cp ../iteration2/affinity_g1_p0_h0.model .
cp ../iteration3/affinity_batch_relu.model .
cp ../iteration6/affinity_0.250_conv4_pool1_grouped0_func1_swap0.model .
#cmds only vary seed
